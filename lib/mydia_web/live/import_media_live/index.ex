defmodule MydiaWeb.ImportMediaLive.Index do
  use MydiaWeb, :live_view

  alias Mydia.{Library, Metadata, Settings}
  alias Mydia.Library.{Scanner, MetadataMatcher, FileGrouper, MediaFile}
  alias MydiaWeb.Live.Authorization
  alias MydiaWeb.ImportMediaLive.Components

  @impl true
  def mount(_params, _session, socket) do
    {:ok, initialize_fresh_session(socket)}
  end

  defp initialize_fresh_session(socket) do
    socket
    |> assign(:page_title, "Import Media")
    |> assign(:import_session, nil)
    |> assign(:step, :select_path)
    |> assign(:scan_path, "")
    |> assign(:selected_library_path, nil)
    |> assign(:scanning, false)
    |> assign(:matching, false)
    |> assign(:importing, false)
    |> assign(:discovered_files, [])
    |> assign(:matched_files, [])
    |> assign(:grouped_files, %{
      series: [],
      movies: [],
      ungrouped: [],
      type_filtered: [],
      sample_filtered: [],
      simple: []
    })
    |> assign(:selected_files, MapSet.new())
    |> assign(:scan_stats, %{
      total: 0,
      matched: 0,
      unmatched: 0,
      skipped: 0,
      orphaned: 0,
      type_filtered: 0,
      sample_filtered: 0
    })
    |> assign(:library_paths, Settings.list_library_paths())
    |> assign(:metadata_config, Metadata.default_relay_config())
    |> assign(:import_progress, %{current: 0, total: 0, current_file: nil})
    |> assign(:import_results, %{success: 0, failed: 0, skipped: 0})
    |> assign(:detailed_results, [])
    |> assign(:editing_file_index, nil)
    |> assign(:edit_form, nil)
    |> assign(:search_query, "")
    |> assign(:search_results, [])
    |> assign(:path_suggestions, [])
    |> assign(:show_path_suggestions, false)
    |> assign(:show_type_filtered, false)
    |> assign(:show_sample_filtered, false)
  end

  @impl true
  def handle_params(params, _url, socket) do
    # Check if there's a session_id in the URL
    case Map.get(params, "session_id") do
      nil ->
        # No session_id - generate a new one and redirect
        session_id = Ecto.UUID.generate()
        {:noreply, push_patch(socket, to: ~p"/import?session_id=#{session_id}")}

      session_id ->
        # Try to load the session
        case Library.get_import_session(session_id) do
          nil ->
            # Session doesn't exist yet - this is a new import
            {:noreply, assign(socket, :session_id, session_id)}

          session ->
            # Session exists - restore it
            {:noreply, assign(restore_session(socket, session), :session_id, session_id)}
        end
    end
  end

  ## Event Handlers

  @impl true
  def handle_event("select_library_path", %{"path_id" => path_id}, socket) do
    with :ok <- Authorization.authorize_import_media(socket) do
      library_path = Enum.find(socket.assigns.library_paths, &(to_string(&1.id) == path_id))

      if library_path do
        # Automatically start scan when library path is selected
        send(self(), {:perform_scan, library_path.path})

        {:noreply,
         socket
         |> assign(:scan_path, library_path.path)
         |> assign(:selected_library_path, library_path)
         |> assign(:scanning, true)
         |> assign(:step, :review)
         |> assign(:discovered_files, [])
         |> assign(:matched_files, [])}
      else
        {:noreply, socket}
      end
    else
      {:unauthorized, socket} -> {:noreply, socket}
    end
  end

  def handle_event("toggle_file_selection", %{"index" => index_str}, socket) do
    index = String.to_integer(index_str)
    selected_files = socket.assigns.selected_files

    selected_files =
      if MapSet.member?(selected_files, index) do
        MapSet.delete(selected_files, index)
      else
        MapSet.put(selected_files, index)
      end

    socket =
      socket
      |> assign(:selected_files, selected_files)
      |> persist_session()

    {:noreply, socket}
  end

  def handle_event("select_all_files", _params, socket) do
    # Select only successfully matched files
    matched_indices =
      socket.assigns.matched_files
      |> Enum.with_index()
      |> Enum.filter(fn {file, _idx} -> file.match_result != nil end)
      |> Enum.map(fn {_file, idx} -> idx end)
      |> MapSet.new()

    socket =
      socket
      |> assign(:selected_files, matched_indices)
      |> persist_session()

    {:noreply, socket}
  end

  def handle_event("deselect_all_files", _params, socket) do
    socket =
      socket
      |> assign(:selected_files, MapSet.new())
      |> persist_session()

    {:noreply, socket}
  end

  def handle_event("toggle_season_selection", %{"indices" => indices_str}, socket) do
    # Parse the comma-separated indices
    indices =
      indices_str
      |> String.split(",")
      |> Enum.map(&String.to_integer/1)
      |> MapSet.new()

    selected_files = socket.assigns.selected_files

    # Check if all episodes in this season are selected
    all_selected = MapSet.subset?(indices, selected_files)

    # Toggle: if all selected, deselect all; otherwise, select all
    selected_files =
      if all_selected do
        MapSet.difference(selected_files, indices)
      else
        MapSet.union(selected_files, indices)
      end

    socket =
      socket
      |> assign(:selected_files, selected_files)
      |> persist_session()

    {:noreply, socket}
  end

  def handle_event("start_import", _params, socket) do
    with :ok <- Authorization.authorize_import_media(socket) do
      if MapSet.size(socket.assigns.selected_files) > 0 do
        send(self(), :perform_import)

        socket =
          socket
          |> assign(:importing, true)
          |> assign(:step, :importing)
          |> assign(
            :import_progress,
            %{current: 0, total: MapSet.size(socket.assigns.selected_files), current_file: nil}
          )
          |> persist_session()

        {:noreply, socket}
      else
        {:noreply, put_flash(socket, :error, "Please select at least one file to import")}
      end
    else
      {:unauthorized, socket} -> {:noreply, socket}
    end
  end

  def handle_event("start_over", _params, socket) do
    # Abandon the current session
    if socket.assigns.import_session do
      Library.abandon_active_import_sessions(socket.assigns.current_user.id)
    end

    socket =
      socket
      |> assign(:import_session, nil)
      |> assign(:step, :select_path)
      |> assign(:scan_path, "")
      |> assign(:selected_library_path, nil)
      |> assign(:discovered_files, [])
      |> assign(:matched_files, [])
      |> assign(:grouped_files, %{
        series: [],
        movies: [],
        ungrouped: [],
        type_filtered: [],
        simple: []
      })
      |> assign(:selected_files, MapSet.new())
      |> assign(:scan_stats, %{
        total: 0,
        matched: 0,
        unmatched: 0,
        skipped: 0,
        orphaned: 0,
        type_filtered: 0
      })
      |> assign(:import_progress, %{current: 0, total: 0, current_file: nil})
      |> assign(:import_results, %{success: 0, failed: 0, skipped: 0})
      |> assign(:detailed_results, [])
      |> assign(:show_type_filtered, false)

    {:noreply, socket}
  end

  def handle_event("cancel", _params, socket) do
    {:noreply, push_navigate(socket, to: ~p"/media")}
  end

  def handle_event("toggle_type_filtered", _params, socket) do
    {:noreply, assign(socket, :show_type_filtered, !socket.assigns.show_type_filtered)}
  end

  def handle_event("toggle_sample_filtered", _params, socket) do
    {:noreply, assign(socket, :show_sample_filtered, !socket.assigns.show_sample_filtered)}
  end

  def handle_event("edit_file", %{"index" => index_str}, socket) do
    index = String.to_integer(index_str)
    matched_file = Enum.at(socket.assigns.matched_files, index)

    if matched_file do
      # Handle both matched and unmatched files
      edit_form =
        if matched_file.match_result do
          # Existing match - populate with current values
          match = matched_file.match_result
          parsed_info = match.parsed_info

          %{
            "title" => match.title,
            "provider_id" => match.provider_id,
            "year" => to_string(match.year || ""),
            "season" => to_string(parsed_info.season || ""),
            "episodes" => Enum.join(parsed_info.episodes || [], ", "),
            "type" => to_string(parsed_info.type)
          }
        else
          # No match - start with empty form but pre-populate with filename hint
          filename = Path.basename(matched_file.file.path, Path.extname(matched_file.file.path))

          %{
            "title" => filename,
            "provider_id" => "",
            "year" => "",
            "season" => "",
            "episodes" => "",
            "type" => "movie"
          }
        end

      search_query = if matched_file.match_result, do: matched_file.match_result.title, else: ""

      {:noreply,
       socket
       |> assign(:editing_file_index, index)
       |> assign(:edit_form, edit_form)
       |> assign(:search_query, search_query)
       |> assign(:search_results, [])}
    else
      {:noreply, socket}
    end
  end

  def handle_event("cancel_edit", _params, socket) do
    {:noreply,
     socket
     |> assign(:editing_file_index, nil)
     |> assign(:edit_form, nil)
     |> assign(:search_query, "")
     |> assign(:search_results, [])}
  end

  def handle_event("save_edit", %{"edit_form" => form_params}, socket) do
    index = socket.assigns.editing_file_index

    if index != nil do
      matched_file = Enum.at(socket.assigns.matched_files, index)

      # Validate form data through changeset
      changeset = validate_edit_form(form_params)

      if changeset.valid? do
        validated_data = Ecto.Changeset.apply_changes(changeset)

        # Build updated or new match result
        updated_match =
          if matched_file.match_result do
            # Update existing match
            Map.merge(matched_file.match_result, %{
              title: validated_data.title,
              provider_id: validated_data.provider_id,
              year: validated_data.year,
              manually_edited: true,
              parsed_info:
                Map.merge(matched_file.match_result.parsed_info, %{
                  season: Map.get(validated_data, :season),
                  episodes: Map.get(validated_data, :episodes, []),
                  type: validated_data.type
                })
            })
          else
            # Create new match for previously unmatched file
            %{
              title: validated_data.title,
              provider_id: validated_data.provider_id,
              year: validated_data.year,
              match_confidence: 1.0,
              manually_edited: true,
              metadata: %{},
              parsed_info: %{
                season: Map.get(validated_data, :season),
                episodes: Map.get(validated_data, :episodes, []),
                type: validated_data.type
              }
            }
          end

        # Update the matched_file in the list
        updated_matched_file = %{matched_file | match_result: updated_match}

        updated_matched_files =
          List.replace_at(socket.assigns.matched_files, index, updated_matched_file)

        # Re-group files to update the hierarchical view
        grouped_files = FileGrouper.group_files(updated_matched_files)

        # Recalculate scan stats if we just matched an unmatched file
        scan_stats =
          if matched_file.match_result == nil do
            %{
              socket.assigns.scan_stats
              | matched: socket.assigns.scan_stats.matched + 1,
                unmatched: socket.assigns.scan_stats.unmatched - 1
            }
          else
            socket.assigns.scan_stats
          end

        flash_message =
          if matched_file.match_result,
            do: "Match updated successfully",
            else: "Match created successfully"

        socket =
          socket
          |> assign(:matched_files, updated_matched_files)
          |> assign(:grouped_files, grouped_files)
          |> assign(:scan_stats, scan_stats)
          |> assign(:editing_file_index, nil)
          |> assign(:edit_form, nil)
          |> assign(:search_query, "")
          |> assign(:search_results, [])
          |> persist_session()
          |> put_flash(:info, flash_message)

        {:noreply, socket}
      else
        {:noreply,
         socket
         |> put_flash(:error, "Please fix the validation errors")
         |> assign(:edit_form, form_params)}
      end
    else
      {:noreply, socket}
    end
  end

  def handle_event("search_series", %{"edit_form" => %{"title" => query}}, socket) do
    perform_search(query, socket)
  end

  def handle_event("search_series", %{"query" => query}, socket) do
    perform_search(query, socket)
  end

  def handle_event("select_search_result", params, socket) do
    # Validate search result params
    changeset = validate_search_result(params)

    if changeset.valid? && socket.assigns.edit_form do
      validated_data = Ecto.Changeset.apply_changes(changeset)

      updated_form =
        socket.assigns.edit_form
        |> Map.put("title", validated_data.title)
        |> Map.put("provider_id", validated_data.provider_id)
        |> Map.put("year", validated_data.year)
        |> Map.put("type", validated_data.type)

      {:noreply,
       socket
       |> assign(:edit_form, updated_form)
       |> assign(:search_results, [])
       |> assign(:search_query, validated_data.title)}
    else
      {:noreply, socket}
    end
  end

  def handle_event("clear_match", %{"index" => index_str}, socket) do
    index = String.to_integer(index_str)
    matched_file = Enum.at(socket.assigns.matched_files, index)

    if matched_file do
      # Clear the match result but keep the file
      updated_matched_file = %{matched_file | match_result: nil}

      updated_matched_files =
        List.replace_at(socket.assigns.matched_files, index, updated_matched_file)

      # Re-group files
      grouped_files = FileGrouper.group_files(updated_matched_files)

      # Remove from selected files if it was selected
      selected_files = MapSet.delete(socket.assigns.selected_files, index)

      socket =
        socket
        |> assign(:matched_files, updated_matched_files)
        |> assign(:grouped_files, grouped_files)
        |> assign(:selected_files, selected_files)
        |> persist_session()
        |> put_flash(:info, "Match cleared")

      {:noreply, socket}
    else
      {:noreply, socket}
    end
  end

  def handle_event("retry_failed_item", %{"index" => index_str}, socket) do
    with :ok <- Authorization.authorize_import_media(socket) do
      index = String.to_integer(index_str)
      failed_item = Enum.at(socket.assigns.detailed_results, index)

      if failed_item && failed_item.status == :failed do
        # Find the original matched file from scan
        matched_file =
          Enum.find(socket.assigns.matched_files, fn mf ->
            mf.file.path == failed_item.file_path
          end)

        if matched_file do
          # Retry the import
          new_result = import_file_with_details(matched_file, socket.assigns.metadata_config)

          # Update the detailed results
          updated_results = List.replace_at(socket.assigns.detailed_results, index, new_result)

          # Recalculate counts
          success_count = Enum.count(updated_results, &(&1.status == :success))
          failed_count = Enum.count(updated_results, &(&1.status == :failed))
          skipped_count = Enum.count(updated_results, &(&1.status == :skipped))

          {:noreply,
           socket
           |> assign(:detailed_results, updated_results)
           |> assign(:import_results, %{
             success: success_count,
             failed: failed_count,
             skipped: skipped_count
           })
           |> put_flash(:info, "Retried import for #{failed_item.file_name}")}
        else
          {:noreply, put_flash(socket, :error, "Could not find original file data")}
        end
      else
        {:noreply, put_flash(socket, :error, "Invalid retry request")}
      end
    else
      {:unauthorized, socket} -> {:noreply, socket}
    end
  end

  def handle_event("retry_all_failed", _params, socket) do
    with :ok <- Authorization.authorize_import_media(socket) do
      # Get all failed items with their indices
      failed_items_with_indices =
        socket.assigns.detailed_results
        |> Enum.with_index()
        |> Enum.filter(fn {result, _idx} -> result.status == :failed end)

      if failed_items_with_indices == [] do
        {:noreply, put_flash(socket, :info, "No failed items to retry")}
      else
        # Retry each failed item
        updated_results =
          Enum.reduce(failed_items_with_indices, socket.assigns.detailed_results, fn {failed_item,
                                                                                      index},
                                                                                     acc_results ->
            # Find the original matched file
            matched_file =
              Enum.find(socket.assigns.matched_files, fn mf ->
                mf.file.path == failed_item.file_path
              end)

            if matched_file do
              new_result = import_file_with_details(matched_file, socket.assigns.metadata_config)
              List.replace_at(acc_results, index, new_result)
            else
              acc_results
            end
          end)

        # Recalculate counts
        success_count = Enum.count(updated_results, &(&1.status == :success))
        failed_count = Enum.count(updated_results, &(&1.status == :failed))
        skipped_count = Enum.count(updated_results, &(&1.status == :skipped))

        {:noreply,
         socket
         |> assign(:detailed_results, updated_results)
         |> assign(:import_results, %{
           success: success_count,
           failed: failed_count,
           skipped: skipped_count
         })
         |> put_flash(:info, "Retried #{length(failed_items_with_indices)} failed items")}
      end
    else
      {:unauthorized, socket} -> {:noreply, socket}
    end
  end

  def handle_event("export_results", _params, socket) do
    # Generate JSON export of detailed results
    export_data = %{
      timestamp: DateTime.utc_now(),
      summary: socket.assigns.import_results,
      results: socket.assigns.detailed_results
    }

    json_data = Jason.encode!(export_data, pretty: true)

    {:noreply,
     socket
     |> push_event("download_export", %{
       filename: "import_results_#{DateTime.utc_now() |> DateTime.to_unix()}.json",
       content: json_data,
       mime_type: "application/json"
     })}
  end

  ## Private Helpers

  defp perform_search(query, socket) do
    if String.trim(query) != "" && String.length(query) >= 2 do
      # Search both movies and TV shows
      config = socket.assigns.metadata_config

      movie_results =
        case Metadata.search(config, query, media_type: :movie) do
          {:ok, results} -> Enum.map(results, &Map.put(&1, :media_type, "movie"))
          _ -> []
        end

      tv_results =
        case Metadata.search(config, query, media_type: :tv_show) do
          {:ok, results} -> Enum.map(results, &Map.put(&1, :media_type, "tv"))
          _ -> []
        end

      # Combine and limit to first 10 results
      search_results = (movie_results ++ tv_results) |> Enum.take(10)

      {:noreply,
       socket
       |> assign(:search_query, query)
       |> assign(:search_results, search_results)}
    else
      {:noreply,
       socket
       |> assign(:search_query, query)
       |> assign(:search_results, [])}
    end
  end

  ## Async Handlers

  @impl true
  def handle_info({:perform_scan, path}, socket) do
    library_path = socket.assigns.selected_library_path
    # Use library-type-specific file extensions
    extensions = Scanner.extensions_for_library_type(library_path && library_path.type)

    case Scanner.scan(path, video_extensions: extensions) do
      {:ok, scan_result} ->
        # Get existing files from database (preload library_path for absolute_path resolution)
        # Only skip files that have valid parent associations (not orphaned)
        existing_files = Library.list_media_files(preload: [:library_path])

        existing_valid_paths =
          existing_files
          |> Enum.reject(&Library.orphaned_media_file?/1)
          |> Enum.map(&MediaFile.absolute_path/1)
          |> Enum.reject(&is_nil/1)
          |> MapSet.new()

        # Build map of orphaned files for re-matching
        orphaned_files_map =
          existing_files
          |> Enum.filter(&Library.orphaned_media_file?/1)
          |> Enum.map(fn file ->
            case MediaFile.absolute_path(file) do
              nil -> nil
              abs_path -> {abs_path, file}
            end
          end)
          |> Enum.reject(&is_nil/1)
          |> Map.new()

        # Filter out files that already have valid associations
        # Include orphaned files for re-matching
        new_files =
          Enum.reject(scan_result.files, fn file ->
            MapSet.member?(existing_valid_paths, file.path)
          end)

        # Track which files are orphaned (for re-matching)
        files_to_match =
          Enum.map(new_files, fn file ->
            orphaned_file = Map.get(orphaned_files_map, file.path)

            Map.put(file, :orphaned_media_file_id, orphaned_file && orphaned_file.id)
          end)

        skipped_count = length(scan_result.files) - length(new_files)
        orphaned_count = map_size(orphaned_files_map)

        # Start matching files
        send(self(), {:match_files, files_to_match})

        {:noreply,
         socket
         |> assign(:scanning, false)
         |> assign(:matching, true)
         |> assign(:discovered_files, scan_result.files)
         |> assign(
           :scan_stats,
           %{
             total: length(files_to_match),
             matched: 0,
             unmatched: 0,
             skipped: skipped_count,
             orphaned: orphaned_count
           }
         )}

      {:error, reason} ->
        {:noreply,
         socket
         |> assign(:scanning, false)
         |> assign(:step, :select_path)
         |> put_flash(:error, "Scan failed: #{format_error(reason)}")}
    end
  end

  def handle_info({:match_files, files}, socket) do
    library_path = socket.assigns.selected_library_path

    # For specialized library types, skip metadata matching entirely
    if specialized_library?(library_path) do
      handle_specialized_library_files(files, socket)
    else
      handle_standard_library_files(files, socket)
    end
  end

  def handle_info(:perform_import, socket) do
    selected_indices = MapSet.to_list(socket.assigns.selected_files)
    selected_files = Enum.map(selected_indices, &Enum.at(socket.assigns.matched_files, &1))

    # Import each file and collect detailed results
    detailed_results =
      Enum.with_index(selected_files)
      |> Enum.map(fn {matched_file, idx} ->
        # Update progress with current file
        file_name = Path.basename(matched_file.file.path)
        send(self(), {:update_import_progress, idx + 1, file_name})

        import_file_with_details(matched_file, socket.assigns.metadata_config)
      end)

    success_count = Enum.count(detailed_results, &(&1.status == :success))
    failed_count = Enum.count(detailed_results, &(&1.status == :failed))
    skipped_count = Enum.count(detailed_results, &(&1.status == :skipped))

    socket =
      socket
      |> assign(:importing, false)
      |> assign(:step, :complete)
      |> assign(:import_results, %{
        success: success_count,
        failed: failed_count,
        skipped: skipped_count
      })
      |> assign(:detailed_results, detailed_results)
      |> persist_session()

    # Mark session as completed
    if socket.assigns.import_session do
      Library.complete_import_session(socket.assigns.import_session)
    end

    {:noreply, socket}
  end

  def handle_info({:update_import_progress, current, current_file}, socket) do
    {:noreply,
     assign(socket, :import_progress, %{
       socket.assigns.import_progress
       | current: current,
         current_file: current_file
     })}
  end

  ## Specialized Library Handling

  defp specialized_library?(nil), do: false
  defp specialized_library?(%{type: type}), do: type in [:music, :books, :adult]

  # Handle files for specialized libraries (music, books, adult)
  # These don't need metadata matching - just create a simple file listing
  defp handle_specialized_library_files(files, socket) do
    # Create matched_files structure without metadata matching
    matched_files =
      Enum.map(files, fn file ->
        %{
          file: file,
          match_result: nil,
          import_status: :pending
        }
      end)

    # Group all files under the "simple" category for specialized libraries
    grouped_files = %{
      series: [],
      movies: [],
      ungrouped: [],
      type_filtered: [],
      simple: matched_files
    }

    # Auto-select all files (user can deselect if needed)
    auto_selected =
      matched_files
      |> Enum.with_index()
      |> Enum.map(fn {_file, idx} -> idx end)
      |> MapSet.new()

    socket =
      socket
      |> assign(:matching, false)
      |> assign(:step, :review)
      |> assign(:matched_files, matched_files)
      |> assign(:grouped_files, grouped_files)
      |> assign(:selected_files, auto_selected)
      |> assign(:scan_stats, %{
        total: length(files),
        matched: length(files),
        unmatched: 0,
        skipped: socket.assigns.scan_stats.skipped,
        type_filtered: 0
      })
      |> persist_session()

    {:noreply, socket}
  end

  # Handle files for standard libraries (movies, series, mixed)
  # These use metadata matching to identify content
  defp handle_standard_library_files(files, socket) do
    library_path = socket.assigns.selected_library_path

    # Match files with TMDB in batches for better UX
    matched_files =
      Enum.map(files, fn file ->
        match_result =
          case MetadataMatcher.match_file(file.path, config: socket.assigns.metadata_config) do
            {:ok, match} -> match
            {:error, _reason} -> nil
          end

        %{
          file: file,
          match_result: match_result,
          import_status: :pending
        }
      end)

    # Filter out files whose media type doesn't match the library type
    {compatible_files, type_filtered_files} =
      filter_by_library_type(matched_files, library_path)

    # Filter out sample files, trailers, and extras
    {regular_files, sample_filtered_files} =
      filter_samples_and_extras(compatible_files)

    # Calculate stats
    matched_count = Enum.count(regular_files, &(&1.match_result != nil))
    unmatched_count = length(regular_files) - matched_count
    type_filtered_count = length(type_filtered_files)
    sample_filtered_count = length(sample_filtered_files)

    # Group files hierarchically (only regular files)
    grouped_files =
      regular_files
      |> FileGrouper.group_files()
      |> Map.put(:type_filtered, type_filtered_files)
      |> Map.put(:sample_filtered, sample_filtered_files)

    # Auto-select files with high confidence matches (using indices in regular_files)
    auto_selected =
      regular_files
      |> Enum.with_index()
      |> Enum.filter(fn {file, _idx} ->
        file.match_result != nil && file.match_result.match_confidence >= 0.8
      end)
      |> Enum.map(fn {_file, idx} -> idx end)
      |> MapSet.new()

    socket =
      socket
      |> assign(:matching, false)
      |> assign(:step, :review)
      |> assign(:matched_files, regular_files)
      |> assign(:grouped_files, grouped_files)
      |> assign(:selected_files, auto_selected)
      |> assign(:scan_stats, %{
        total: length(files),
        matched: matched_count,
        unmatched: unmatched_count,
        skipped: socket.assigns.scan_stats.skipped,
        type_filtered: type_filtered_count,
        sample_filtered: sample_filtered_count
      })
      |> persist_session()

    {:noreply, socket}
  end

  # Filters out sample files, trailers, and extras based on parsed_info detection
  defp filter_samples_and_extras(matched_files) do
    Enum.split_with(matched_files, fn matched_file ->
      case matched_file.match_result do
        nil ->
          # No match result - can't determine if it's a sample, keep it
          true

        match ->
          parsed_info = match.parsed_info
          # Keep if NOT a sample, trailer, or extra
          not (parsed_info.is_sample or parsed_info.is_trailer or parsed_info.is_extra)
      end
    end)
  end

  ## Library Type Filtering

  # Filters files by library type compatibility
  # Returns {compatible_files, type_filtered_files}
  defp filter_by_library_type(matched_files, nil), do: {matched_files, []}

  defp filter_by_library_type(matched_files, library_path) do
    case library_path.type do
      :mixed ->
        # Mixed libraries accept all media types
        {matched_files, []}

      :series ->
        # Series-only library: filter out movies
        Enum.split_with(matched_files, fn matched_file ->
          case matched_file.match_result do
            nil -> true
            match -> match.parsed_info.type != :movie
          end
        end)

      :movies ->
        # Movies-only library: filter out TV shows
        Enum.split_with(matched_files, fn matched_file ->
          case matched_file.match_result do
            nil -> true
            match -> match.parsed_info.type != :tv_show
          end
        end)

      _ ->
        # Unknown library type, don't filter
        {matched_files, []}
    end
  end

  ## Session Management

  defp restore_session(socket, session) do
    session_data = session.session_data || %{}
    library_paths = Settings.list_library_paths()

    # Restore the selected library path from the scan_path
    selected_library_path =
      if session.scan_path do
        Enum.find(library_paths, fn lp -> lp.path == session.scan_path end)
      else
        nil
      end

    socket
    |> assign(:page_title, "Import Media")
    |> assign(:import_session, session)
    |> assign(:step, session.step)
    |> assign(:scan_path, session.scan_path || "")
    |> assign(:selected_library_path, selected_library_path)
    |> assign(:scanning, false)
    |> assign(:matching, false)
    |> assign(:importing, session.step == :importing)
    |> assign(:discovered_files, Map.get(session_data, "discovered_files", []))
    |> assign(
      :matched_files,
      restore_matched_files(Map.get(session_data, "matched_files", []))
    )
    |> assign(
      :grouped_files,
      Map.get(
        session_data,
        "grouped_files",
        %{"series" => [], "movies" => [], "ungrouped" => [], "type_filtered" => []}
      )
      |> atomize_grouped_files()
    )
    |> assign(
      :selected_files,
      Map.get(session_data, "selected_files", []) |> MapSet.new()
    )
    |> assign(
      :scan_stats,
      if session.scan_stats && session.scan_stats != %{} do
        atomize_keys(session.scan_stats)
      else
        %{total: 0, matched: 0, unmatched: 0, skipped: 0, orphaned: 0, type_filtered: 0}
      end
    )
    |> assign(:library_paths, library_paths)
    |> assign(:metadata_config, Metadata.default_relay_config())
    |> assign(
      :import_progress,
      if session.import_progress && session.import_progress != %{} do
        atomize_keys(session.import_progress)
      else
        %{current: 0, total: 0, current_file: nil}
      end
    )
    |> assign(
      :import_results,
      if session.import_results && session.import_results != %{} do
        atomize_keys(session.import_results)
      else
        %{success: 0, failed: 0, skipped: 0}
      end
    )
    |> assign(
      :detailed_results,
      Map.get(session_data, "detailed_results", [])
      |> atomize_detailed_results()
    )
    |> assign(:editing_file_index, nil)
    |> assign(:edit_form, nil)
    |> assign(:search_query, "")
    |> assign(:search_results, [])
    |> assign(:path_suggestions, [])
    |> assign(:show_path_suggestions, false)
    |> assign(:show_type_filtered, false)
  end

  defp persist_session(socket) do
    session_id = socket.assigns[:session_id]
    user_id = socket.assigns.current_user.id

    # If no session_id in assigns, skip persistence
    if session_id == nil do
      socket
    else
      session_data = %{
        "discovered_files" => socket.assigns.discovered_files,
        "matched_files" => prepare_matched_files_for_storage(socket.assigns.matched_files),
        "grouped_files" => prepare_grouped_files_for_storage(socket.assigns.grouped_files),
        "selected_files" => MapSet.to_list(socket.assigns.selected_files),
        "detailed_results" => socket.assigns.detailed_results
      }

      attrs = %{
        step: socket.assigns.step,
        scan_path: socket.assigns.scan_path,
        session_data: session_data,
        scan_stats: socket.assigns.scan_stats,
        import_progress: socket.assigns.import_progress,
        import_results: socket.assigns.import_results
      }

      # Check if session already exists
      case Library.get_import_session(session_id) do
        nil ->
          # Create new session with the specific ID
          case Library.create_import_session(
                 Map.merge(attrs, %{id: session_id, user_id: user_id})
               ) do
            {:ok, new_session} ->
              assign(socket, :import_session, new_session)

            {:error, _changeset} ->
              socket
          end

        existing_session ->
          # Update existing session
          case Library.update_import_session(existing_session, attrs) do
            {:ok, updated_session} ->
              assign(socket, :import_session, updated_session)

            {:error, _changeset} ->
              socket
          end
      end
    end
  end

  defp prepare_matched_files_for_storage(matched_files) do
    Enum.map(matched_files, fn matched_file ->
      %{
        "file" => to_storable_map(matched_file.file),
        "match_result" => to_storable_map(matched_file.match_result),
        "import_status" => matched_file.import_status
      }
    end)
  end

  defp prepare_grouped_files_for_storage(grouped_files) do
    %{
      "series" => Enum.map(grouped_files.series || [], &to_storable_map/1),
      "movies" => Enum.map(grouped_files.movies || [], &to_storable_map/1),
      "ungrouped" => Enum.map(grouped_files.ungrouped || [], &to_storable_map/1),
      "type_filtered" => Enum.map(grouped_files[:type_filtered] || [], &to_storable_map/1)
    }
  end

  # Recursively convert structs to maps for storage
  # Converts all atom keys to strings and handles special types like DateTime
  defp to_storable_map(nil), do: nil

  defp to_storable_map(%DateTime{} = dt) do
    DateTime.to_iso8601(dt)
  end

  defp to_storable_map(%_{} = struct) do
    struct
    |> Map.from_struct()
    |> Enum.map(fn {k, v} -> {to_string(k), to_storable_map(v)} end)
    |> Map.new()
  end

  defp to_storable_map(map) when is_map(map) do
    Enum.map(map, fn {k, v} -> {to_string(k), to_storable_map(v)} end)
    |> Map.new()
  end

  defp to_storable_map(list) when is_list(list) do
    Enum.map(list, &to_storable_map/1)
  end

  defp to_storable_map(value), do: value

  defp restore_matched_files(stored_files) do
    Enum.map(stored_files, fn stored_file ->
      %{
        file: atomize_keys(stored_file["file"]),
        match_result:
          if stored_file["match_result"] do
            atomize_match_result(stored_file["match_result"])
          else
            nil
          end,
        import_status: String.to_atom(stored_file["import_status"] || "pending")
      }
    end)
  end

  defp atomize_keys(nil), do: nil

  defp atomize_keys(map) when is_map(map) do
    Map.new(map, fn
      {key, value} when is_binary(key) ->
        # Use String.to_atom/1 for persistence data since we control the keys
        {String.to_atom(key), atomize_keys(value)}

      {key, value} ->
        {key, atomize_keys(value)}
    end)
  end

  defp atomize_keys(list) when is_list(list), do: Enum.map(list, &atomize_keys/1)
  defp atomize_keys(value), do: value

  defp atomize_match_result(nil), do: nil

  defp atomize_match_result(match) when is_map(match) do
    match
    |> Map.new(fn
      {"parsed_info", parsed_info} when is_map(parsed_info) ->
        {:parsed_info, atomize_keys(parsed_info)}

      {"metadata", metadata} when is_map(metadata) ->
        {:metadata, atomize_keys(metadata)}

      # provider_type needs to be converted back to an atom
      {"provider_type", value} when is_binary(value) ->
        {:provider_type, String.to_atom(value)}

      {key, value} when is_binary(key) ->
        {String.to_atom(key), value}

      {key, value} ->
        {key, value}
    end)
  end

  defp atomize_grouped_files(grouped) when is_map(grouped) do
    %{
      series: atomize_keys(Map.get(grouped, "series", [])),
      movies: atomize_keys(Map.get(grouped, "movies", [])),
      ungrouped: atomize_keys(Map.get(grouped, "ungrouped", [])),
      type_filtered: restore_matched_files(Map.get(grouped, "type_filtered", []))
    }
  end

  defp atomize_detailed_results(results) when is_list(results) do
    Enum.map(results, &atomize_keys/1)
  end

  ## Private Helpers

  defp import_file_with_details(%{match_result: nil, file: file}, _config) do
    # Check if this file belongs to a specialized library
    # If so, import without metadata matching
    library_paths = Settings.list_library_paths()

    {library_path_id, relative_path} =
      calculate_relative_path_for_import(file.path, library_paths)

    library_path = Enum.find(library_paths, &(&1.id == library_path_id))

    if library_path && library_path.type in [:music, :books, :adult] do
      # Import file for specialized library without metadata
      case Library.create_scanned_media_file(%{
             relative_path: relative_path,
             library_path_id: library_path_id,
             size: file.size,
             verified_at: DateTime.utc_now()
           }) do
        {:ok, _media_file} ->
          %{
            file_path: file.path,
            file_name: Path.basename(file.path),
            status: :success,
            media_item_title: Path.basename(file.path, Path.extname(file.path)),
            error_message: nil,
            action_taken: "Added to #{library_type_label(library_path.type)} library",
            metadata: %{size: file.size, library_type: library_path.type}
          }

        {:error, changeset} ->
          error_msg = format_changeset_errors(changeset)

          %{
            file_path: file.path,
            file_name: Path.basename(file.path),
            status: :failed,
            media_item_title: nil,
            error_message: "Database error: #{error_msg}",
            action_taken: nil,
            metadata: %{size: file.size}
          }
      end
    else
      # Standard library file without metadata match - report failure
      %{
        file_path: file.path,
        file_name: Path.basename(file.path),
        status: :failed,
        media_item_title: nil,
        error_message: "No metadata match found for this file",
        action_taken: nil,
        metadata: %{size: file.size}
      }
    end
  end

  defp import_file_with_details(%{file: file, match_result: match_result}, config) do
    # Check if this file is orphaned and needs re-matching
    media_file_result =
      if file[:orphaned_media_file_id] do
        # Use existing orphaned media file - don't update it yet
        # The enricher will handle associating it with the media item
        try do
          media_file = Library.get_media_file!(file.orphaned_media_file_id)
          {:ok, media_file}
        rescue
          _ -> {:error, :not_found}
        end
      else
        # Create new media file record with relative path
        # Find matching library_path and calculate relative_path
        library_paths = Settings.list_library_paths()

        {library_path_id, relative_path} =
          calculate_relative_path_for_import(file.path, library_paths)

        Library.create_scanned_media_file(%{
          relative_path: relative_path,
          library_path_id: library_path_id,
          size: file.size,
          verified_at: DateTime.utc_now()
        })
      end

    case media_file_result do
      {:ok, media_file} ->
        # Enrich with metadata
        case Library.MetadataEnricher.enrich(match_result,
               config: config,
               media_file_id: media_file.id
             ) do
          {:ok, media_item} ->
            %{
              file_path: file.path,
              file_name: Path.basename(file.path),
              status: :success,
              media_item_title: match_result.title,
              error_message: nil,
              action_taken: build_success_message(match_result, file[:orphaned_media_file_id]),
              metadata: %{
                size: file.size,
                media_item_id: media_item.id,
                year: match_result.year,
                type: match_result.parsed_info.type
              }
            }

          {:error, reason} ->
            %{
              file_path: file.path,
              file_name: Path.basename(file.path),
              status: :failed,
              media_item_title: match_result.title,
              error_message: "Failed to enrich metadata: #{format_error(reason)}",
              action_taken: nil,
              metadata: %{size: file.size}
            }
        end

      {:error, changeset} ->
        error_msg =
          case changeset do
            %Ecto.Changeset{errors: errors} ->
              errors
              |> Enum.map(fn {field, {msg, _}} -> "#{field} #{msg}" end)
              |> Enum.join(", ")

            other ->
              format_error(other)
          end

        %{
          file_path: file.path,
          file_name: Path.basename(file.path),
          status: :failed,
          media_item_title: match_result.title,
          error_message: "Database error: #{error_msg}",
          action_taken: nil,
          metadata: %{size: file.size}
        }
    end
  rescue
    error ->
      %{
        file_path: file.path,
        file_name: Path.basename(file.path),
        status: :failed,
        media_item_title: match_result && match_result.title,
        error_message: "Unexpected error: #{Exception.message(error)}",
        action_taken: nil,
        metadata: %{size: file.size}
      }
  end

  defp build_success_message(match_result, _is_orphaned) do
    media_type =
      case match_result.parsed_info.type do
        :tv_show ->
          if match_result.parsed_info.season do
            "TV Show S#{String.pad_leading("#{match_result.parsed_info.season}", 2, "0")}"
          else
            "TV Show"
          end

        _ ->
          "Movie"
      end

    "Imported #{media_type}: '#{match_result.title}'"
  end

  defp library_type_label(:music), do: "Music"
  defp library_type_label(:books), do: "Books"
  defp library_type_label(:adult), do: "Adult"
  defp library_type_label(type), do: to_string(type)

  defp format_changeset_errors(%Ecto.Changeset{errors: errors}) do
    errors
    |> Enum.map(fn {field, {msg, _}} -> "#{field} #{msg}" end)
    |> Enum.join(", ")
  end

  defp format_changeset_errors(other), do: format_error(other)

  defp format_error(:not_found), do: "Directory not found"
  defp format_error(:not_directory), do: "Path is not a directory"
  defp format_error(:permission_denied), do: "Permission denied"
  defp format_error(reason), do: inspect(reason)

  # Validation functions for form data
  defp validate_edit_form(params) do
    types = %{
      title: :string,
      provider_id: :string,
      year: :integer,
      season: :integer,
      episodes: {:array, :integer},
      type: :string
    }

    {%{}, types}
    |> Ecto.Changeset.cast(params, [:title, :provider_id, :year, :season, :type])
    |> cast_episode_list(params["episodes"])
    |> Ecto.Changeset.validate_required([:title, :type])
    |> Ecto.Changeset.validate_inclusion(:type, ["movie", "tv_show"])
  end

  defp validate_search_result(params) do
    types = %{
      provider_id: :string,
      title: :string,
      year: :string,
      type: :string
    }

    {%{}, types}
    |> Ecto.Changeset.cast(params, [:provider_id, :title, :year, :type])
    |> Ecto.Changeset.validate_required([:provider_id, :title, :type])
    |> normalize_media_type()
  end

  defp cast_episode_list(changeset, episodes_param) when is_binary(episodes_param) do
    case parse_episode_list(episodes_param) do
      {:ok, episodes} ->
        Ecto.Changeset.put_change(changeset, :episodes, episodes)

      {:error, _message} ->
        Ecto.Changeset.add_error(changeset, :episodes, "invalid episode format")
    end
  end

  defp cast_episode_list(changeset, _), do: changeset

  defp normalize_media_type(changeset) do
    case Ecto.Changeset.get_change(changeset, :type) do
      "tv" ->
        Ecto.Changeset.put_change(changeset, :type, :tv_show)

      "movie" ->
        Ecto.Changeset.put_change(changeset, :type, :movie)

      type when is_binary(type) ->
        case String.to_existing_atom(type) do
          atom when atom in [:tv_show, :movie] ->
            Ecto.Changeset.put_change(changeset, :type, atom)

          _ ->
            Ecto.Changeset.add_error(changeset, :type, "invalid media type")
        end

      _ ->
        changeset
    end
  rescue
    ArgumentError ->
      Ecto.Changeset.add_error(changeset, :type, "invalid media type")
  end

  # Parse helper functions for edit form
  defp parse_episode_list(""), do: {:ok, []}

  defp parse_episode_list(value) when is_binary(value) do
    episodes =
      value
      |> String.split(",")
      |> Enum.map(&String.trim/1)
      |> Enum.reject(&(&1 == ""))
      |> Enum.map(fn ep_str ->
        case Integer.parse(ep_str) do
          {int, ""} -> {:ok, int}
          _ -> {:error, ep_str}
        end
      end)

    errors = Enum.filter(episodes, &match?({:error, _}, &1))

    if errors == [] do
      {:ok, Enum.map(episodes, fn {:ok, ep} -> ep end)}
    else
      {:error, "Invalid episode number format"}
    end
  end

  defp parse_episode_list(value) when is_list(value), do: {:ok, value}

  # Calculates the relative path and library_path_id for an absolute file path
  # Returns {library_path_id, relative_path}
  defp calculate_relative_path_for_import(absolute_path, library_paths) do
    # Find the library_path that this file belongs to (longest matching prefix)
    matching_path =
      library_paths
      |> Enum.filter(fn lp -> String.starts_with?(absolute_path, lp.path) end)
      |> Enum.max_by(fn lp -> String.length(lp.path) end, fn -> nil end)

    case matching_path do
      nil ->
        require Logger

        Logger.warning("No matching library path found for file during import",
          path: absolute_path
        )

        # Return nil for both - the changeset will handle validation
        {nil, nil}

      library_path ->
        # Calculate relative path by removing the library path prefix
        relative_path =
          absolute_path
          |> String.replace_prefix(library_path.path, "")
          |> String.trim_leading("/")

        {library_path.id, relative_path}
    end
  end

  # Helper functions for specialized library UI
  defp library_type_icon(:music), do: "hero-musical-note"
  defp library_type_icon(:books), do: "hero-book-open"
  defp library_type_icon(:adult), do: "hero-eye-slash"
  defp library_type_icon(:series), do: "hero-tv"
  defp library_type_icon(:movies), do: "hero-film"
  defp library_type_icon(:mixed), do: "hero-square-3-stack-3d"
  defp library_type_icon(_), do: "hero-folder"

  defp library_type_header_class(:music), do: "bg-success/10"
  defp library_type_header_class(:books), do: "bg-warning/10"
  defp library_type_header_class(:adult), do: "bg-error/10"
  defp library_type_header_class(_), do: "bg-base-200/50"

  defp library_type_plural(:music), do: "Music Files"
  defp library_type_plural(:books), do: "Books"
  defp library_type_plural(:adult), do: "Files"
  defp library_type_plural(_), do: "Files"
end
